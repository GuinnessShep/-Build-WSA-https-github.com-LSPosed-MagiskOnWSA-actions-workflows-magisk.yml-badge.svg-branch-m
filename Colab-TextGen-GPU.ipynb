{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuinnessShep/-Build-WSA-https-github.com-LSPosed-MagiskOnWSA-actions-workflows-magisk.yml-badge.svg-branch-m/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM text generation notebook for Google Colab\n",
        "\n",
        "This notebook uses [https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) to run the [Pygmalion](https://huggingface.co/models?search=pygmalion) conversational models in chat mode.\n",
        "\n",
        "Run all the cells and a public gradio URL will appear at the bottom in around 5 minutes.\n",
        "\n",
        "https://status.gradio.app/\n",
        "\n",
        "## Parameters\n",
        "\n",
        "* **save_logs_to_google_drive**: saves your chat logs, characters, and softprompts to Google Drive automatically, so that they will persist across sessions.\n",
        "* **text_streaming**: streams the text output in real time instead of waiting for the full response to be completed.\n",
        "* **cai_chat**: makes the interface look like Character.AI. Otherwise, it looks like a standard WhatsApp-like chat.\n",
        "* **load_in_8bit**: loads the model with 8-bit precision, reducing the GPU memory usage by half. This allows you to use the full 2048 prompt length without running out of memory, at a small accuracy and speed cost.\n",
        "* **activate_silero_text_to_speech**: responses will be audios instead of text. There are 118 voices available (`en_0` to `en_117`), which can be set in the \"Extensions\" tab of the interface. You can find samples here: [Silero samples](https://oobabooga.github.io/silero-samples/).\n",
        "* **activate_sending_pictures**: adds a menu for sending pictures to the bot, which are automatically captioned using BLIP.\n",
        "* **activate_character_bias**: an extension that adds an user-defined, hidden string at the beginning of the bot's reply with the goal of biasing the rest of the response.\n",
        "* **chat_language**: if different than English, activates automatic translation using Google Translate, allowing you to communicate with the bot in a different language.\n",
        "\    "n",
    "* **scripting_language**: if different than English, activates automatic translation using Google Translate for scripting prompts (softprompts).\n",
    "\n",
    "## Usage\n",
    "\n",
    "Click on the generated Gradio link at the bottom to start chatting with the model.\n",
    "\n",
    "## Notes\n",
    "\n",
    "The performance of the notebook will depend on your current GPU allocation. You might need to adjust the `max_prompt_length` parameter according to your GPU's memory capacity.\n",
    "\n",
    "For more information, visit the [text-generation-webui GitHub repository](https://github.com/oobabooga/text-generation-webui).\n",
    "\n",
    "## Credit\n",
    "\n",
    "This notebook is based on the work by [oobabooga](https://github.com/oobabooga) and has been modified to include the new models as requested.\n",
    "\n",
    "Happy chatting!"
  ],
  "metadata": {
    "id": "intro"
  }
},
{
  "cell_type": "code",
  "metadata": {
    "id": "imports"
  },
  "source": [
    "!pip install -q gradio transformers pygmalion-model-6b torch blip-utils\n",
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "torch.set_printoptions(precision=10)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beoswindvip/pygmalion-6b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mayaeary/pygmalion-6b-4bit-128g\")"
  ],
  "execution_count": 1,
  "outputs": [
    {
      "output_type": "stream",
      "name": "stderr",
      "text": [
        "  Building wheel for pygmalion-model-6b (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
      ]
    }
  ]
},
{
  "cell_type": "code",
  "metadata": {
    "id": "generate_function"
  },
  "source": [
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, num_return_sequences=1, max_length=2048, no_repeat_ngram_size=2, temperature=0.5)\n",
    "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return generated_text"
  ],
  "execution_count": 2,
  "outputs": []
},
{
  "cell_type": "code",
  "metadata": {
    "id": "gradio_interface"
  },
  "source": [
    "iface = gr.Interface(\n",
    "    fn=generate_text, \n",
    "    inputs=gr.inputs.Textbox(lines=20, label=\"Input Prompt\"), \n",
    "    outputs=gr.outputs.Textbox(lines=20, label=\"Generated Text\"), \n",
    "    title=\"Text Generation with Pygmalion-6b Models\",\n",
    "    description=\"Generate text using the Pygmalion-6b models on Hugging Face.\"\n",
    ")\n",
    "iface.launch(share=True)"
  ],
  "execution_count": 3,
  "outputs": [
    {
      "output_type": "stream",
      "name": "stdout",
      "text": [
        "Running locally at: http://127.0.0.1:7860/\n",
        "This share link will expire in 24 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted (NEW!)\n",
        "Running on External URL: https://<generated-url>.gradio.app\n"
      ]
    }
  ]
}
}

